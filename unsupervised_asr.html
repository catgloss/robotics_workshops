<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Unsupervised ASR Tutorial &mdash; Robotics Workshops Cluster Guide  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Datasets" href="datasets.html" />
    <link rel="prev" title="Robotics Workshops Cluster Guide" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Robotics Workshops Cluster Guide
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Unsupervised ASR Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#environment-setup">1. Environment Setup</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#running-scripts-on-your-system">1.1 Running Scripts on Your System</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-scripts-on-vector-cluster">1.2 Running Scripts On Vector Cluster</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#training-guide">2. Training Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#audio-preprocessing-librispeech">2.1 Audio preprocessing (LibriSpeech)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#text-preprocessing-librispeech">2.2 Text preprocessing (LibriSpeech)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gan-training">2.3 GAN Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#inference-guide">3. Inference Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="#iterative-self-training-kaldi-lm-decoding">4. Iterative self-training + Kaldi LM-decoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="#applied-changes">5. Applied Changes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">Datasets</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Robotics Workshops Cluster Guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Unsupervised ASR Tutorial</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/unsupervised_asr.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="unsupervised-asr-tutorial">
<h1>Unsupervised ASR Tutorial<a class="headerlink" href="#unsupervised-asr-tutorial" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://github.com/pytorch/fairseq/tree/main/examples/wav2vec/unsupervised">Wav2Veq-U</a>
is a model provided inside the the <a class="reference external" href="https://github.com/pytorch/fairseq">Fairseq</a> toolkit
that makes speech recognition possible by using unpaired audio and text
data, hence called unsupervised. This tutorial is a complementary set of instructions in addition to the
original
<a class="reference external" href="https://github.com/pytorch/fairseq/tree/main/examples/wav2vec/unsupervised#readme">README</a> of the Wav2Vec-U project.</p>
<p>The main steps in running the Wav2Veq-U model are as follows:</p>
<ul class="simple">
<li><p>Preparation of speech representations and text data</p></li>
<li><p>Generative adversarial (GAN) training</p></li>
<li><p>Decoding using Viterbi or Kaldi decoders</p></li>
<li><p>Iterative self-training + Kaldi LM-decoding</p></li>
</ul>
<section id="environment-setup">
<h2>1. Environment Setup<a class="headerlink" href="#environment-setup" title="Permalink to this headline"></a></h2>
<p>Running the Wav2Veq-U project requires installing multiple dependencies.
The list of the dependencies are as follows:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/zhenghuatan/rVADfast">RVadfast</a></p></li>
<li><p><a class="reference external" href="https://github.com/kpu/kenlm">KenLM</a></p></li>
<li><p><a class="reference external" href="https://www.intel.com/content/www/us/en/develop/documentation/get-started-with-mkl-for-dpcpp/top.html">Intel MKL</a></p></li>
<li><p><a class="reference external" href="https://github.com/pykaldi/pykaldi">PyKaldi</a></p></li>
<li><p><a class="reference external" href="https://github.com/flashlight/flashlight">Flashlight Python binding</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/fairseq">Fairseq toolkit</a></p></li>
</ul>
<p>In addition to the above-mentioned requirements, a comprehensive list of
required Python packages can be found in
<a class="reference external" href="https://github.com/VectorInstitute/ASR/blob/main/unsupervised_asr/requirements.txt">requirements.txt</a>.
(The project has been tested on <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">=</span> <span class="pre">3.6.9</span></code>.)</p>
<p>In order to simplify the process of installing dependencies we have
provided a <a class="reference external" href="https://github.com/VectorInstitute/ASR/blob/main/unsupervised_asr/Dockerfile">Dockerfile</a> to build the unsupervised Docker image.
The image can be found on <a class="reference external" href="https://hub.docker.com/r/convaiasr/unsupervised">Docker Hub</a>.</p>
<p>Depending on where and how want to run the project, there are different
approaches towards installing the dependencies. Here, we consider three
cases.</p>
<p><strong>1. Running the project on a machine where you have root access and can install all the packages directly</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>In this case you can follow the same steps as described in the <a class="reference external" href="https://github.com/VectorInstitute/ASR/blob/main/unsupervised_asr/Dockerfile">Dockerfile</a> to make sure that you have all the packages installed and the required environment variables are set. In the Dockerfile, after installing the system requirements and Python packages, the dependencies are installed in the same order as mentioned above.</p></li>
<li><p>Note that due to some discrepancies, some changes has been made to the Fairseq repository. It is recommended that you use the fork of Fairseq repository <a class="reference external" href="https://github.com/mahshidaln/fairseq">here</a>, similar to the what has been done in the Dockerfile.</p></li>
</ul>
</div></blockquote>
<p><strong>2. Running the project inside a Docker container</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>In this case you can pull the Docker image using the following command.</p></li>
</ul>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker pull convaiasr/unsupervised
</pre></div>
</div>
</div></blockquote>
</div></blockquote>
<dl>
<dt><strong>3. Running the project on Vector cluster inside a Singularity Container</strong></dt><dd><ul class="simple">
<li><p>In this case you can use Singularity to pull the Docker image.</p></li>
</ul>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># loading singularity</span>
module load singularity-ce-3.8.2
<span class="c1"># pulling the image</span>
singularity pull docker://convaiasr/unsupervised
</pre></div>
</div>
</div></blockquote>
</dd>
</dl>
<section id="running-scripts-on-your-system">
<h3>1.1 Running Scripts on Your System<a class="headerlink" href="#running-scripts-on-your-system" title="Permalink to this headline"></a></h3>
<ul>
<li><p>If you are running the project on your own system directly, you can run the exact scripts mentioned in Training Guide and Inference Guide.</p></li>
<li><p>If you are running the project inside the Docker container, you can run the container to have an interactive bash session and then run the commands while binding directories on the host system to save the results files.</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker run -v /on/my/host/1:/on/the/container/1 <span class="se">\</span>
           -v /on/my/host/2:/on/the/container/2 <span class="se">\</span>
           unsupervised /bin/bash
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</section>
<section id="running-scripts-on-vector-cluster">
<h3>1.2 Running Scripts On Vector Cluster<a class="headerlink" href="#running-scripts-on-vector-cluster" title="Permalink to this headline"></a></h3>
<ul>
<li><p>In order to run the project on the Vector cluster using singularity you need to submit a job on SLURM scheduler. To do so, you need to create a shell script (e.g. <code class="docutils literal notranslate"><span class="pre">job.sh</span></code>) and submit the job.</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch job.sh
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Below is a sample of the job script that runs Singularity on Vector cluster for training and inference step. You can use the following sample and replace the command string following -c with the actual commands that are required for every step of data preprocessing by binding the appropriate directories.</p></li>
</ul>
<blockquote>
<div><p>We have already trained the GAN on LibriSpeech dataset. We used 100 hours of audio data and we used 10% of the LibriSpeech language model corpus as the text data. The preprocessed data and the checkpoints can be found on the Vector cluster directory <code class="docutils literal notranslate"><span class="pre">/ssd03/projects/convai/asr/</span></code>.</p>
<p>The checkpoints to the trained GAN model can be found in <code class="docutils literal notranslate"><span class="pre">/ssd03/projects/convai/asr/gan_checkpoints</span></code> while preprocessed text and audio data are stored in <code class="docutils literal notranslate"><span class="pre">/ssd03/projects/convai/asr/librispeech_preprocessed</span></code></p>
<p>You can keep the config (.yaml) files for training and inference on the host system and edit them directly or pass the required config options as arguments in the following script.
The sample config files can be found <a class="reference external" href="https://github.com/VectorInstitute/ASR/blob/main/unsupervised_asr/config">here</a>. (It is assumed that the data directory <code class="docutils literal notranslate"><span class="pre">/ssd03/projects/convai/asr/</span></code> from the host is binded to <code class="docutils literal notranslate"><span class="pre">/unsupervised/data</span></code> inside the container)</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --ntasks=1</span>
<span class="c1">#SBATCH --mem=24G</span>
<span class="c1">#SBATCH -c 8</span>
<span class="c1">#SBATCH --partition=t4v2</span>
<span class="c1">#SBATCH --export=ALL</span>
<span class="c1">#SBATCH --output=%x.%j.log</span>
<span class="c1">#SBATCH --gres=gpu:1</span>

hostname
nvidia-smi

module load cuda-11.0
module load singularity-ce-3.8.2

singularity <span class="nb">exec</span> --nv -B /path/to/data/dir:/unsupervised/data <span class="se">\</span>
                    -B /path/to/config/dir:/unsupervised/config <span class="se">\</span>
                    -B /path/to/outputs/dir:/unsupervised/outputs unsupervised_latest.sif <span class="se">\</span>
                    bash -c <span class="se">\</span>
                    <span class="s2">&quot;cd /fairseq/examples/wav2vec/unsupervised &amp;&amp; \</span>
<span class="s2">                    PYTHONPATH=/fairseq PREFIX=w2v_unsup_gan_xp fairseq-hydra-train \</span>
<span class="s2">                    -m --config-dir config/gan \ # or /unsupervised/config/gan if config files are on the host system</span>
<span class="s2">                    --config-name w2vu \</span>
<span class="s2">                    hydra.output_subdir=/unsupervised/outputs \</span>
<span class="s2">                    hydra.run.dir=/unsupervised/outputs \</span>
<span class="s2">                    hydra.sweep.dir=/unsupervised/outputs \</span>
<span class="s2">                    task.data=/path/to/features/precompute_unfiltered_pca512_cls128_mean_pooled \</span>
<span class="s2">                    task.text_data=/path/to/data/phones \</span>
<span class="s2">                    task.kenlm_path=/path/to/data/phones/kenlm.phn.o4.bin \</span>
<span class="s2">                    common.user_dir=/fairseq/examples/wav2vec/unsupervised \</span>
<span class="s2">                    common.seed=0&quot;</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --ntasks=1</span>
<span class="c1">#SBATCH --mem=24G</span>
<span class="c1">#SBATCH -c 8</span>
<span class="c1">#SBATCH --partition=t4v2</span>
<span class="c1">#SBATCH --export=ALL</span>
<span class="c1">#SBATCH --output=%x.%j.log</span>
<span class="c1">#SBATCH --gres=gpu:1</span>

hostname
nvidia-smi

module load cuda-11.0
module load singularity-ce-3.8.2

singularity <span class="nb">exec</span> --nv -B /path/to/data/dir:/unsupervised/data <span class="se">\</span>
                    -B /path/to/config/dir:/unsupervised/config <span class="se">\</span>
                    -B /path/to/outputs/dir:/unsupervised/outputs unsupervised_latest.sif <span class="se">\</span>
                    bash -c <span class="se">\</span>
                    <span class="s2">&quot;cd /fairseq/examples/wav2vec/unsupervised &amp;&amp; \</span>
<span class="s2">                    python w2vu_generate.py --config-dir config/generate --config-name kaldi_wrd \ #or /unsupervised/config/generate if config files are on the host system</span>
<span class="s2">                    hydra.output_subdir=/unsupervised/outputs \</span>
<span class="s2">                    hydra.run.dir=/unsupervised/outputs \</span>
<span class="s2">                    hydra.sweep.dir=/unsupervised/outputs \</span>
<span class="s2">                    fairseq.common.user_dir=/fairseq/examples/wav2vec/unsupervised \</span>
<span class="s2">                    fairseq.task.data=/path/to/features/precompute_unfiltered_pca512_cls128_mean \</span>
<span class="s2">                    fairseq.common_eval.path=/path/to/gan/checkpoint.pt \</span>
<span class="s2">                    results_path=/where/to/save/transcriptions \</span>
<span class="s2">                    kaldi_decoder_config.hlg_graph_path=path_to/HLG \</span>
<span class="s2">                    kaldi_decoder_config.output_dict=path_to/kaldi_dict&quot;</span>
</pre></div>
</div>
</div></blockquote>
</div></blockquote>
<ul>
<li><p>Throughout the tutorial there are multiple references to some environment variables such as $FAIRSEQ_ROOT, $KENLM_ROOT, $RVAD_ROOT. These values need to be replaced with the paths set inside the container which are as follows.</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">FAIRSEQ_ROOT</span><span class="o">=</span>/fairseq
<span class="nv">KENLM_ROOT</span><span class="o">=</span>/opt/kenlm
<span class="nv">RVAD_ROOT</span><span class="o">=</span>/opt/rVADfast
<span class="nv">KALDI_ROOT</span><span class="o">=</span>/opt/pykaldi/tools/kaldi
<span class="nv">FASTTEXT</span><span class="o">=</span>/opt/fasttext
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Also set all other paths to input/output files/directories with respect to the binding that you set for the container.</p></li>
<li><p>If you wish to make changes to the source code you might want to keep a clone of the fairseq repository on the host system and bind the unsupervised project directory to its corresponding location inside the container.</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>singularity <span class="nb">exec</span> -B /path/to/host/fairseq/examples/wav2vec/unsupervised:/fairseq/examples/wav2vec/unsupervised ...
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</section>
</section>
<section id="training-guide">
<h2>2. Training Guide<a class="headerlink" href="#training-guide" title="Permalink to this headline"></a></h2>
<p>Tha main training step in this project is to train a GAN model on unpaired audio and text. But before that, both audio and text data need to be preprocessed.</p>
<p>Here we elaborate on LibriSpeech preprocessing. You can also find instructions on <a class="reference external" href="https://catalog.ldc.upenn.edu/LDC93s1">TIMIT</a> preprocessing in the original <a class="reference external" href="https://github.com/pytorch/fairseq/tree/main/examples/wav2vec/unsupervised#readme">README</a>.</p>
<section id="audio-preprocessing-librispeech">
<h3>2.1 Audio preprocessing (LibriSpeech)<a class="headerlink" href="#audio-preprocessing-librispeech" title="Permalink to this headline"></a></h3>
<dl>
<dt>Audio preprocessing starts with splitting the dataset, creating a manifest file, detecting silent segments and removing them using rVAD.</dt><dd><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># create a manifest file for the set original of audio files</span>
python <span class="nv">$FAIRSEQ_ROOT</span>/examples/wav2vec/wav2vec_manifest.py /dir/to/save/audio/files --ext wav --dest /path/to/new/train.tsv --valid-percent <span class="m">0</span>

python scripts/vads.py -r <span class="nv">$RVAD_ROOT</span> &lt; /path/to/train.tsv &gt; train.vads

python scripts/remove_silence.py --tsv /path/to/train.tsv --vads train.vads --out /dir/to/save/audio/files

python <span class="nv">$FAIRSEQ_ROOT</span>/examples/wav2vec/wav2vec_manifest.py /dir/to/save/audio/files --ext wav --dest /path/to/new/train.tsv --valid-percent <span class="m">0</span>.01
</pre></div>
</div>
</dd>
</dl>
<p>Note that the silence removal should also be applied to the test set before the prepare_audio script is called. (The original silence removal scripts only consider train and valid sets)</p>
<p>The next steps are about capturing the audio representations using the wav2veq 2.0 model, applying PCA, detecting the segments using k-means clustering and applying mean pooling. All these steps are followed in the <code class="docutils literal notranslate"><span class="pre">prepare_audio.sh</span></code>.</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>zsh scripts/prepare_audio.sh /dir/with/<span class="o">{</span>train,test,valid<span class="o">}</span>.tsv /output/dir /path/to/wav2vec2/model.pt <span class="m">512</span> <span class="m">14</span>
</pre></div>
</div>
</div></blockquote>
<p>Note that if you have splits different than train/valid/test, you will need to modify this script. The last two arguments are the PCA dimensionality and the 0-based index of the layer from which to extract representations.</p>
<p>The major steps in the script are as follows:</p>
<ol class="arabic simple">
<li><p>Extracting audio representation using Wav2Vec 2.0 (the pretrained models can be found on the <a class="reference external" href="https://github.com/pytorch/fairseq/tree/main/examples/wav2vec#pre-trained-models">github page</a>).</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Note that you need to download a checkpoint that has not been finetuned e.g. Wav2Vec 2.0 Large (LV-60).</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Training the clustering model on the train set.</p></li>
<li><p>Applying the clustering to all audio subsets.</p></li>
<li><p>Training PCA on the train set.</p></li>
<li><p>Applying PCA on all the audio subsets.</p></li>
<li><p>Calculating the means of the PCA results.</p></li>
<li><p>Applying mean pooling.</p></li>
</ol>
<p>The .wrd, .ltr, .phn files also need to be generated for the data splits (train, valid, test) since they are used in evaluation and error rate calculation.</p>
<ul>
<li><p>The .wrd files includes the real transcription of audio files in the order the are manifested in the {train, valid, test}.tsv after removing silence (the manifest script reorders the files).</p></li>
<li><p>The .ltr files can be achieved using wrd_to_ltr.py:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python <span class="nv">$FAIRSEQ_ROOT</span>/examples/wav2vec/unsupervised/scripts/wrd_to_ltr.py --compact &lt; <span class="nv">$target_dir</span>/<span class="si">${</span><span class="nv">split</span><span class="si">}</span>.wrd &gt; <span class="nv">$target_dir</span>/<span class="si">${</span><span class="nv">split</span><span class="si">}</span>.ltr
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>The .phn files can be generated using a phonemizer. (for English language G2P is the recommended phonemizer):</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python <span class="nv">$FAIRSEQ_ROOT</span>/examples/wav2vec/unsupervised/scripts/g2p_wrd_to_phn.py --compact &lt; <span class="nv">$target_dir</span>/<span class="si">${</span><span class="nv">split</span><span class="si">}</span>.wrd &gt; <span class="nv">$target_dir</span>/<span class="si">${</span><span class="nv">split</span><span class="si">}</span>.phn
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</section>
<section id="text-preprocessing-librispeech">
<h3>2.2 Text preprocessing (LibriSpeech)<a class="headerlink" href="#text-preprocessing-librispeech" title="Permalink to this headline"></a></h3>
<p>Text preprocessing is done through the <code class="docutils literal notranslate"><span class="pre">prepare_text.sh</span></code> script:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>zsh scripts/prepare_text.sh language /path/to/text/file /output/dir <span class="m">1000</span> g2p /path/to/fasttext/lid/model
</pre></div>
</div>
</div></blockquote>
<p>The stpes in the script are as follows:</p>
<ol class="arabic simple">
<li><p>Normalizing and filtering text by removing numbers, punctuations, and words from other languages.</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>The sixth argument is the path to pre-trained fasttext LID models that can be downloaded <a class="reference external" href="https://fasttext.cc/docs/en/language-identification.html">here</a>.</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Fairseq preprocessing to generate the dict.txt file for words.</p></li>
<li><p>Applying the phonemizer and generating the phone.txt.</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>The fifth argument is which phonemizer to use. Supported values are <a class="reference external" href="http://espeak.sourceforge.net/">espeak</a>, <a class="reference external" href="https://github.com/espeak-ng/espeak-ng">espeak-ng</a>, and <a class="reference external" href="https://github.com/Kyubyong/g2p">G2P</a> (english only).</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p>Generating lexicon.lst.</p></li>
<li><p>Fairseq processing on the phonemes to filter the phonemes that are seen less than a threshold and generate phones/dict.txt.</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>The threshold is set by the the fourth argument i.e. the minimum number observations of phones to keep. If your text corpus is small, you might want to reduce this number</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="6">
<li><p>Generating the filtered lexicon according to the phonemes dictionary.</p></li>
<li><p>Inserting &lt;SIL&gt; into the phone transcription and generate lm.phones.filtered.txt.</p></li>
<li><p>Adding SIL to the phone dictionary.</p></li>
<li><p>Fairseq preprocessing using the updated phone dictionary.</p></li>
<li><p>Generating a 4-gram word lamguage model using kenlm and generating arpa and bin files.</p></li>
<li><p>Creating a fst decoding model using Kaldi. (The outputs will be used for word decoding).</p></li>
<li><p>Generating a 4-gram phoneme language model using kenlm and generating arpa and bin files.</p></li>
<li><p>Creating a fst decoding model using Kaldi. (The outputs will be used for phoneme decoding).</p></li>
</ol>
</section>
<section id="gan-training">
<h3>2.3 GAN Training<a class="headerlink" href="#gan-training" title="Permalink to this headline"></a></h3>
<p>After the unpaired text and audio representations are prepared, they are used to to train the GAN model. The configuration file for GAN training can be found in the <code class="docutils literal notranslate"><span class="pre">config/gan</span></code> directory.</p>
<p>Launching GAN training on top of preprocessed features, with default hyperparameters can be done with:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">PREFIX</span><span class="o">=</span>w2v_unsup_gan_xp
<span class="nv">TASK_DATA</span><span class="o">=</span>/path/to/features/precompute_unfiltered_pca512_cls128_mean_pooled
<span class="nv">TEXT_DATA</span><span class="o">=</span>/path/to/data/phones  <span class="c1"># path to fairseq-preprocessed GAN data (phones dir)</span>
<span class="nv">KENLM_PATH</span><span class="o">=</span>/path/to/data/phones/kenlm.phn.o4.bin  <span class="c1"># KenLM 4-gram phoneme language model (LM data = GAN data here)</span>

<span class="nv">PYTHONPATH</span><span class="o">=</span><span class="nv">$FAIRSEQ_ROOT</span> <span class="nv">PREFIX</span><span class="o">=</span><span class="nv">$PREFIX</span> fairseq-hydra-train <span class="se">\</span>
    -m --config-dir config/gan <span class="se">\</span>
    --config-name w2vu <span class="se">\</span>
    task.data<span class="o">=</span><span class="si">${</span><span class="nv">TASK_DATA</span><span class="si">}</span> <span class="se">\</span>
    task.text_data<span class="o">=</span><span class="si">${</span><span class="nv">TEXT_DATA</span><span class="si">}</span> <span class="se">\</span>
    task.kenlm_path<span class="o">=</span><span class="si">${</span><span class="nv">KENLM_PATH</span><span class="si">}</span> <span class="se">\</span>
    common.user_dir<span class="o">=</span><span class="si">${</span><span class="nv">FAIRSEQ_ROOT</span><span class="si">}</span>/examples/wav2vec/unsupervised <span class="se">\</span>
    model.code_penalty<span class="o">=</span><span class="m">2</span> or <span class="m">4</span> <span class="se">\</span>
    model.gradient_penalty<span class="o">=</span><span class="m">1</span>.5 or <span class="m">2</span>.0 <span class="se">\</span>
    model.smoothness_weight<span class="o">=</span><span class="m">0</span>.5 or <span class="m">0</span>.75 or <span class="m">1</span>.0 <span class="se">\</span>
    common.seed<span class="o">=</span>range<span class="o">(</span><span class="m">0</span>,5<span class="o">)</span>
</pre></div>
</div>
</div></blockquote>
</section>
</section>
<section id="inference-guide">
<h2>3. Inference Guide<a class="headerlink" href="#inference-guide" title="Permalink to this headline"></a></h2>
<p>Once we find the best checkpoint (chosen using unsupervised metric that combined language model perplexity and vocabulary usage), we can use it to generate phone labels (or word labels with an appropriate Kaldi decoder):</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python w2vu_generate.py --config-dir config/generate --config-name viterbi <span class="se">\</span>
fairseq.common.user_dir<span class="o">=</span><span class="si">${</span><span class="nv">FAIRSEQ_ROOT</span><span class="si">}</span>/examples/wav2vec/unsupervised <span class="se">\</span>
fairseq.task.data<span class="o">=</span>/path/to/dir/with/features <span class="se">\</span>
fairseq.common_eval.path<span class="o">=</span>/path/to/gan/checkpoint <span class="se">\</span>
fairseq.dataset.gen_subset<span class="o">=</span>valid <span class="nv">results_path</span><span class="o">=</span>/where/to/save/transcriptions
</pre></div>
</div>
</div></blockquote>
<ul>
<li><p>Decoding can be done either without a language model (e.g. Viterbi decoding) or with a language model (Kaldi Decoding). Decoding without a LM works best on the same adjacent-mean-pooled features that the gan was trained on, while decoding with LM works better on features before the adjacent time-step mean-pooling step (without the “_pooled” suffix).</p></li>
<li><p>The config for Viterbi decoding to generate phone labels can be found in <code class="docutils literal notranslate"><span class="pre">config/generate/viterbi.yaml</span></code>.</p></li>
<li><p>Kaldi decoder can be applied before and after self-training for phoneme and word decoding. The full list of Config parameters can be found in the w2lu.generate.py. When you want to use the Kaldi decoder, you should make sure that Kaldi decoder config are included in the config file. You can find out more about the config in kaldi_decoder.py and kaldi_initializer.py. There two necessary config items:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hlg_graph_path: path_to/HLG.phn.kenlm.wrd.o40003.fst <span class="o">(</span><span class="k">for</span> word decoding<span class="o">)</span> or path to HLG.phn.lm.phones.filtered.06.fst <span class="o">(</span><span class="k">for</span> phoneme decoding<span class="o">)</span>
output_dict: path_to/kaldi_dict.kenlm.wrd.o40003.txt or path_to/kaldi_dict.lm.phones.filtered.06.txt
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>The config for Kaldi decoding to generate phone labels can be found in <code class="docutils literal notranslate"><span class="pre">config/generate/kaldi_phn.yaml</span></code></p></li>
<li><p>The config for Kaldi decoding to generate phone labels can be found in <code class="docutils literal notranslate"><span class="pre">config/generate/kaldi_wrd.yaml</span></code></p></li>
<li><p>Note that the targets argument indicates if the output are supposed to be phonemes or words and in case of phonemes (phn) the WER is actually the PER.</p></li>
<li><p>Note that during inference the dict.phn.txt should be present in the audio features directory.</p></li>
</ul>
</section>
<section id="iterative-self-training-kaldi-lm-decoding">
<h2>4. Iterative self-training + Kaldi LM-decoding<a class="headerlink" href="#iterative-self-training-kaldi-lm-decoding" title="Permalink to this headline"></a></h2>
<p>For self-training you can refer to the original <a class="reference external" href="https://github.com/pytorch/fairseq/tree/main/examples/wav2vec/unsupervised#readme">README</a>.</p>
<p>Further instructions will be added soon.</p>
</section>
<section id="applied-changes">
<h2>5. Applied Changes<a class="headerlink" href="#applied-changes" title="Permalink to this headline"></a></h2>
<p>A few changes were made to the forked version of the repository to prevent errors:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">capture_output</span></code> is removed from python subprocess so the <code class="docutils literal notranslate"><span class="pre">kaldi_init.py</span></code> of fairseq needed to be updated. (replaced with <code class="docutils literal notranslate"><span class="pre">std_out</span> <span class="pre">=</span> <span class="pre">PIPE</span></code> and <code class="docutils literal notranslate"><span class="pre">std_err</span> <span class="pre">=</span> <span class="pre">PIPE</span></code> where necessary)</p></li>
<li><p>Kaldi logging in <code class="docutils literal notranslate"><span class="pre">add-self-loop-simple.cc</span></code> was removed.</p></li>
<li><p>According to latest updates of fairseq the optimization <code class="docutils literal notranslate"><span class="pre">amsgrad</span></code> option caused errors and was removed from the training config file.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Robotics Workshops Cluster Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="datasets.html" class="btn btn-neutral float-right" title="Datasets" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Vector AI Engineering.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>